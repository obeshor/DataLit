{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning Text\n",
    "\n",
    "Teaching a computer accurately understand word-context has been an unsolved problem for a long time. Words with the same meaning can exist in a variety of expressions, and there are even new words born every day.\n",
    "\n",
    "To solve this problem, a variety of research directions are under way which require huge text datasets for their respective purpose.\n",
    " \n",
    "Cleaning text-data is a typical pre-processing task for data science and machine learning.\n",
    "\n",
    "It consists of getting rid of the less useful parts of text through stopword removal, dealing with capitalization, special characters and other details.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1. Sneak peek into the data\n",
    "Take a look at the data: explore its main characteristics like size and structure to see how sentences, paragraphs, text are built.\n",
    "\n",
    "Understand how much of this data is useful for your needs.\n",
    "Review the text to see what exactly might help.\n",
    "In the case of Metamorphosis:\n",
    "\n",
    "There are no obvious typos or spelling mistakes.\n",
    "There’s punctuation like commas, apostrophes, quotes, question marks, and more.\n",
    "Overall it is simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2. Whitespace/Punctuation/Normalize Case\n",
    "There are two ways\n",
    "\n",
    "- do it Manually\n",
    "- use the NLTK library\n",
    "Although we usually prefer the latter, let’s learn how to do it manually to study cleaning text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### < Manually >\n",
    "We can do cleaning text manually in just a few lines of python code. Read, share, and change them to lowercase in three major steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load text\n",
    "filename = 'metamorphosis_clean.txt'\n",
    "file = open(filename, 'rt')\n",
    "text = file.read()\n",
    "file.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Tokenization\n",
    "Tokenization describes splitting paragraphs into sentences, or sentences into individual words.\n",
    "Sentences can be split into individual words and punctuation through a similar process. Most commonly we split words between white spaces.\n",
    "There can be problems when a word is abbreviated, truncated or is possessive especially in the case of names that use punctuation (like O’Neil).\n",
    "Split by Whitespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ï»¿One', 'morning,', 'when', 'Gregor', 'Samsa', 'woke', 'from', 'troubled', 'dreams,', 'he', 'found', 'himself', 'transformed', 'in', 'his', 'bed', 'into', 'a', 'horrible', 'vermin.', 'He', 'lay', 'on', 'his', 'armour-like', 'back,', 'and', 'if', 'he', 'lifted', 'his', 'head', 'a', 'little', 'he', 'could', 'see', 'his', 'brown', 'belly,', 'slightly', 'domed', 'and', 'divided', 'by', 'arches', 'into', 'stiff', 'sections.', 'The', 'bedding', 'was', 'hardly', 'able', 'to', 'cover', 'it', 'and', 'seemed', 'ready', 'to', 'slide', 'off', 'any', 'moment.', 'His', 'many', 'legs,', 'pitifully', 'thin', 'compared', 'with', 'the', 'size', 'of', 'the', 'rest', 'of', 'him,', 'waved', 'about', 'helplessly', 'as', 'he', 'looked.', '\"What\\'s', 'happened', 'to', 'me?\"', 'he', 'thought.', 'It', \"wasn't\", 'a', 'dream.', 'His', 'room,', 'a', 'proper', 'human']\n"
     ]
    }
   ],
   "source": [
    "# load text\n",
    "filename = 'metamorphosis_clean.txt'\n",
    "file = open(filename, 'rt')\n",
    "text = file.read()\n",
    "file.close()\n",
    "# split into words by white space\n",
    "words = text.split()\n",
    "print(words[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ï', 'One', 'morning', 'when', 'Gregor', 'Samsa', 'woke', 'from', 'troubled', 'dreams', 'he', 'found', 'himself', 'transformed', 'in', 'his', 'bed', 'into', 'a', 'horrible', 'vermin', 'He', 'lay', 'on', 'his', 'armour', 'like', 'back', 'and', 'if', 'he', 'lifted', 'his', 'head', 'a', 'little', 'he', 'could', 'see', 'his', 'brown', 'belly', 'slightly', 'domed', 'and', 'divided', 'by', 'arches', 'into', 'stiff', 'sections', 'The', 'bedding', 'was', 'hardly', 'able', 'to', 'cover', 'it', 'and', 'seemed', 'ready', 'to', 'slide', 'off', 'any', 'moment', 'His', 'many', 'legs', 'pitifully', 'thin', 'compared', 'with', 'the', 'size', 'of', 'the', 'rest', 'of', 'him', 'waved', 'about', 'helplessly', 'as', 'he', 'looked', 'What', 's', 'happened', 'to', 'me', 'he', 'thought', 'It', 'wasn', 't', 'a', 'dream', 'His']\n"
     ]
    }
   ],
   "source": [
    "# Split By Words\n",
    "# load text\n",
    "filename = 'metamorphosis_clean.txt'\n",
    "file = open(filename, 'rt')\n",
    "text = file.read()\n",
    "file.close()\n",
    "# split based on words only\n",
    "import re\n",
    "words = re.split(r'\\W+', text)\n",
    "print(words[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ï»¿One', 'morning', 'when', 'Gregor', 'Samsa', 'woke', 'from', 'troubled', 'dreams', 'he', 'found', 'himself', 'transformed', 'in', 'his', 'bed', 'into', 'a', 'horrible', 'vermin', 'He', 'lay', 'on', 'his', 'armourlike', 'back', 'and', 'if', 'he', 'lifted', 'his', 'head', 'a', 'little', 'he', 'could', 'see', 'his', 'brown', 'belly', 'slightly', 'domed', 'and', 'divided', 'by', 'arches', 'into', 'stiff', 'sections', 'The', 'bedding', 'was', 'hardly', 'able', 'to', 'cover', 'it', 'and', 'seemed', 'ready', 'to', 'slide', 'off', 'any', 'moment', 'His', 'many', 'legs', 'pitifully', 'thin', 'compared', 'with', 'the', 'size', 'of', 'the', 'rest', 'of', 'him', 'waved', 'about', 'helplessly', 'as', 'he', 'looked', 'Whats', 'happened', 'to', 'me', 'he', 'thought', 'It', 'wasnt', 'a', 'dream', 'His', 'room', 'a', 'proper', 'human']\n"
     ]
    }
   ],
   "source": [
    "#Split by Whitespace and Remove Punctuation\n",
    "\n",
    "# load text\n",
    "filename = 'metamorphosis_clean.txt'\n",
    "file = open(filename, 'rt')\n",
    "text = file.read()\n",
    "file.close()\n",
    "# split into words by white space\n",
    "words = text.split()\n",
    "# remove punctuation from each word\n",
    "import string\n",
    "table = str.maketrans('', '', string.punctuation)\n",
    "stripped = [w.translate(table) for w in words]\n",
    "print(stripped[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Capitalization\n",
    "Text often has a variety of capitalizations reflecting the beginning of sentences and proper nouns emphasis. The most common approach is to reduce everything to lower case for simplicity but it is important to remember that some words, like “US” to “us”, can change meanings when reduced to the lower case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ï»¿one', 'morning,', 'when', 'gregor', 'samsa', 'woke', 'from', 'troubled', 'dreams,', 'he', 'found', 'himself', 'transformed', 'in', 'his', 'bed', 'into', 'a', 'horrible', 'vermin.', 'he', 'lay', 'on', 'his', 'armour-like', 'back,', 'and', 'if', 'he', 'lifted', 'his', 'head', 'a', 'little', 'he', 'could', 'see', 'his', 'brown', 'belly,', 'slightly', 'domed', 'and', 'divided', 'by', 'arches', 'into', 'stiff', 'sections.', 'the', 'bedding', 'was', 'hardly', 'able', 'to', 'cover', 'it', 'and', 'seemed', 'ready', 'to', 'slide', 'off', 'any', 'moment.', 'his', 'many', 'legs,', 'pitifully', 'thin', 'compared', 'with', 'the', 'size', 'of', 'the', 'rest', 'of', 'him,', 'waved', 'about', 'helplessly', 'as', 'he', 'looked.', '\"what\\'s', 'happened', 'to', 'me?\"', 'he', 'thought.', 'it', \"wasn't\", 'a', 'dream.', 'his', 'room,', 'a', 'proper', 'human']\n"
     ]
    }
   ],
   "source": [
    "filename = 'metamorphosis_clean.txt'\n",
    "file = open(filename, 'rt')\n",
    "text = file.read()\n",
    "file.close()\n",
    "# split into words by white space\n",
    "words = text.split()\n",
    "# convert to lower case\n",
    "words = [word.lower() for word in words]\n",
    "print(words[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### < NLTK >\n",
    "The Natural Language Toolkit, or NLTK for short, is a Python library written for working and modeling text.\n",
    "\n",
    "It provides a high-level api to flexibly implement a variety of cleaning methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Install NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already up-to-date: nltk in c:\\users\\obeshor\\anaconda3\\lib\\site-packages (3.4)\n",
      "Requirement already satisfied, skipping upgrade: six in c:\\users\\obeshor\\anaconda3\\lib\\site-packages (from nltk) (1.12.0)\n",
      "Requirement already satisfied, skipping upgrade: singledispatch in c:\\users\\obeshor\\anaconda3\\lib\\site-packages (from nltk) (3.4.0.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install -U nltk\n",
    "!python -m nltk.downloader all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Tokenization\n",
    "Split into Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "filename = 'metamorphosis_clean.txt'\n",
    "file = open(filename, 'rt')\n",
    "text = file.read()\n",
    "file.close()\n",
    "# split into words\n",
    "from nltk.tokenize import word_tokenize\n",
    "tokens = word_tokenize(text)\n",
    "print(tokens[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Filter Out Punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "filename = 'metamorphosis_clean.txt'\n",
    "file = open(filename, 'rt')\n",
    "text = file.read()\n",
    "file.close()\n",
    "# split into words\n",
    "from nltk.tokenize import word_tokenize\n",
    "tokens = word_tokenize(text)\n",
    "# remove all tokens that are not alphabetic\n",
    "words = [word for word in tokens if word.isalpha()]\n",
    "print(words[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3. Stopwords/Stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Filter out Stopwords and Pipelines\n",
    "A majority of the words in a given text are connecting parts of a sentence rather than showing subjects, objects or intent. Word like “the” or “and” can be removed by comparing text to a list of stopwords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "filename = 'metamorphosis_clean.txt'\n",
    "file = open(filename, 'rt')\n",
    "text = file.read()\n",
    "file.close()\n",
    "# split into words\n",
    "from nltk.tokenize import word_tokenize\n",
    "tokens = word_tokenize(text)\n",
    "# convert to lower case\n",
    "tokens = [w.lower() for w in tokens]\n",
    "# remove punctuation from each word\n",
    "import string\n",
    "table = str.maketrans('', '', string.punctuation)\n",
    "stripped = [w.translate(table) for w in tokens]\n",
    "# remove remaining tokens that are not alphabetic\n",
    "words = [word for word in stripped if word.isalpha()]\n",
    "# filter out stop words\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "words = [w for w in words if not w in stop_words]\n",
    "print(words[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Stemming\n",
    "Stemming is a process where words are reduced to a root by removing inflection through dropping unnecessary characters, usually a suffix. There are several stemming models, including Porter and Snowball. But there is a danger of “over-stemming” were words like “universe” and “university” are reduced to the same root of “univers”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "filename = 'metamorphosis_clean.txt'\n",
    "file = open(filename, 'rt')\n",
    "text = file.read()\n",
    "file.close()\n",
    "# split into words\n",
    "from nltk.tokenize import word_tokenize\n",
    "tokens = word_tokenize(text)\n",
    "# stemming of words\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "porter = PorterStemmer()\n",
    "stemmed = [porter.stem(word) for word in tokens]\n",
    "print(stemmed[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After stemming, ['morning', 'troubled'] converted to ['morn','troubl'] by PorterStemmer.\n",
    "\n",
    "You can also see that the tokens converted to lowercase.\n",
    "\n",
    "Finally, we have cleaned text data reducing words to their root by stemmin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4. Other tools\n",
    "### 1. Lemmatization\n",
    "Lemmatization is also an alternative to removing inflection. By determining the part of text and utilizing WordNet’s lexical database of English, it can get better results.\n",
    "\n",
    "It is a more accurate but slower. Stemming may be more useful in queries for databases whereas Lemmatization may work much better when trying to determine text sentiment.\n",
    "\n",
    "### 2. Word Embedding/Text Vectors\n",
    "Word embedding is the modern way of representing words as vectors. The aim of word embeddings is to find a series of high dimensionality vectors (one for each word) that represent the relation of words in such a way that semantically related words are ‘close together’ in that high dimensional space. Word2Vec and GloVe are the most common models for converting text to vectors. Often, T-SNE (as well as PCA) is used to reduce the dimensionality enough to display as a 2 or 3 dimensional graph. Check out this example of T-SNE applied to word embeddings."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
